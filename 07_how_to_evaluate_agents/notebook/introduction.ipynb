{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf275f8",
   "metadata": {},
   "source": [
    "# Introduction To Evaluation\n",
    "Evaluation is a quantitative way of measuring the performance of an LLM powered application\n",
    "\n",
    "### Why is it important\n",
    "- LLM are non deterministic which means their behavior is not predictable and a small change in prompt, the base LLM, the Input or some configuration can significantly change the output\n",
    "- Evaluation provides a structured way to identify failures, compare changes across different versions of your application\n",
    "- This help you easily iterate and scale your AI application more reliably\n",
    "\n",
    "### Evaluation components\n",
    "Evaluation is made up of three main component\n",
    "1. Dataset: This is the collection of test input and reference output\n",
    "2. Target Function: A target function defines what your are evaluating. For example\n",
    "    - You can evaluation a single node\n",
    "    - You can evaluate a new prompt\n",
    "    - You can evaluate a part of your application\n",
    "    - You can evaluate end to end application\n",
    "\n",
    "3. Evaluators: This are function for scoring outputs. \n",
    "    - *NOTE:* This can either be **online evaluator** or **offline evaluator**\n",
    "\n",
    "\n",
    "![Eval Conceptual](../static/Eval.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ad444",
   "metadata": {},
   "source": [
    "# Lets run a simple evaluation to test the correctness of LLM responses\n",
    "\n",
    "## 1. Create a Dataset\n",
    "A dataset is a collection of examples used for evaluating an application\n",
    "- An example is a pair of test input and test output\n",
    "\n",
    "### a. Create examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103dc154",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the capital city of Kenya?\"},\n",
    "        \"outputs\": {\"answer\": \"The capital city of Kenya is Nairobi.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Who developed the theory of relativity?\"},\n",
    "        \"outputs\": {\"answer\": \"The theory of relativity was developed by Albert Einstein.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the largest planet in our solar system?\"},\n",
    "        \"outputs\": {\"answer\": \"The largest planet in our solar system is Jupiter.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"In which year did World War II end?\"},\n",
    "        \"outputs\": {\"answer\": \"World War II ended in 1945.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the square root of 144?\"},\n",
    "        \"outputs\": {\"answer\": \"The square root of 144 is 12.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Who wrote the play 'Romeo and Juliet'?\"},\n",
    "        \"outputs\": {\"answer\": \"The play 'Romeo and Juliet' was written by William Shakespeare.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the chemical symbol for gold?\"},\n",
    "        \"outputs\": {\"answer\": \"The chemical symbol for gold is Au.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Which continent is the Sahara Desert located in?\"},\n",
    "        \"outputs\": {\"answer\": \"The Sahara Desert is located in Africa.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How many sides does a hexagon have?\"},\n",
    "        \"outputs\": {\"answer\": \"A hexagon has six sides.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the freezing point of water in Celsius?\"},\n",
    "        \"outputs\": {\"answer\": \"The freezing point of water is 0 degrees Celsius.\"}\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29690fc",
   "metadata": {},
   "source": [
    "### Programmatically create a dataset in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d15357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"agent_evaluation\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name, description=\"Dataset for evaluating agent responses\")\n",
    "    \n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(\n",
    "        dataset_id = dataset.id,\n",
    "        examples = dataset_examples\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68624a27",
   "metadata": {},
   "source": [
    "## 2. Define Target Function (What you are evaluating)\n",
    "A target function contains what you are evaluating\n",
    "\n",
    "- In our case we are going to evaluate the entire agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce598350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.agent import graph\n",
    "\n",
    "def target_function(inputs:dict):\n",
    "    \n",
    "    response = graph.invoke(inputs)\n",
    "    \n",
    "    return {\"answer\": response['answer']}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8916b4f",
   "metadata": {},
   "source": [
    "# 3. Define Evaluator\n",
    "Evaluators are functions that score how well your application performs on a particular example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e690ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "# Grade output schema\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\n",
    "\n",
    "\n",
    "# Grade prompt\n",
    "correctness_instructions = \"\"\"\n",
    "You are a teacher grading a quiz. \n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
    "(2) Ensure that the student answer does not contain any conflicting statements.\n",
    "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
    "\n",
    "Correctness:\n",
    "A correctness value of True means that the student's answer meets all of the criteria.\n",
    "A correctness value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "Avoid simply stating the correct answer at the outset.\n",
    "\"\"\"\n",
    "\n",
    "# Grader LLM\n",
    "grader_llm = init_chat_model(model=\"gpt-4o\").with_structured_output(\n",
    "    CorrectnessGrade\n",
    ")\n",
    "\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"An evaluator for answer accuracy\"\"\"\n",
    "    \n",
    "    \n",
    "    answers = f\"\"\"\\\n",
    "        QUESTION: {inputs['question']}\n",
    "        GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
    "        STUDENT ANSWER: {outputs['answer']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run evaluator\n",
    "    grade = grader_llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": correctness_instructions},\n",
    "            {\"role\": \"user\", \"content\": answers},\n",
    "        ]\n",
    "    )\n",
    "    return grade[\"correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692e9e4",
   "metadata": {},
   "source": [
    "# 4. Run and View Results\n",
    "Run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b895aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'agent-evaluation-a36b1d94' at:\n",
      "https://smith.langchain.com/o/5e26199c-44b7-5d71-a174-0781dc496380/datasets/7649f8d9-60f7-48c5-a40d-c2ed1ecde1cc/compare?selectedSessions=4577ffd6-3ce7-4858-98bb-3220ba64a8b6\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:25,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        correctness,\n",
    "        # can add multiple evaluators here\n",
    "    ],\n",
    "    experiment_prefix=\"agent-evaluation\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
